# -*- coding: utf-8 -*-
"""algoritmo_simulazione_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ql9qIDnSTeexloLaxq0eF9jjJezQpg6C
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random as rn
from sklearn import preprocessing
import math
from scipy.stats import multinomial
from sklearn.cluster import KMeans
from scipy.stats import multivariate_normal
from scipy.special import logit, expit
from scipy.stats import multivariate_normal
from scipy.special import factorial
from collections import defaultdict
from algorithm_alpha import algorithm_alpha
from sklearn.metrics import roc_curve, auc
from scipy.stats import multinomial
import statsmodels.api as sm
from statsmodels.regression.mixed_linear_model import MixedLM

def all_mydmultinom(V, prob, v):
      result = np.zeros(len(V))
      for j in range(len(V)):
        if V.iloc[j] == 1:
          esito = [V.iloc[j], 0]
        else:
          esito = [V.iloc[j], 1]
        result[j] = multinomial.pmf(esito, n=1, p=[prob[0], 1-prob[0]])
      return result

def cross_log(pi, y):
  interm = []
  for i in range(len(y)):
    interm.append(math.log((pi[i] ** y[i]) * ((1 - pi[i]) ** (1 - y[i]) )))
  return interm

def poisson_log_loss(lam, y):
    log_loss = []
    for i in range(len(y)):
      log_loss = y[i] * math.log(lam[i]) + lam[i] + math.log(math.factorial(y[i]))
    return log_loss

def all_mydmultinom_log(V, prob, v):
    if v == 1:
      result = np.zeros(len(V))
      for j in range(len(V)):
        if V.iloc[j] == 1:
          esito = [V.iloc[j], 0]
        else:
          esito = [V.iloc[j], 1]
        result[j] = np.log(multinomial.pmf(esito, n=1, p=[prob[0], 1-prob[0]]))
    else :
      result = np.zeros(V.shape[0])
      for i in range(v):
        for j in range(len(V.iloc[:,i])):
          if V.iloc[j,i] == 1:
            esito = [V.iloc[j,i], 0]
          else:
            esito = [V.iloc[j,i], 1]
          result[j] += np.log(multinomial.pmf(esito, n=1, p=[prob[i], 1-prob[i]]))
    return result

def mymult3(w, arr1, U):
    result = np.zeros((U.shape[1], U.shape[1]))
    for i in range(arr1.shape[0]):
        result += w[i] * np.outer(arr1.iloc[i, :], arr1.iloc[i, :])
    return result

def mymult4(w, V):
    result = np.sum(w * V, axis=0)
    return result

def initialize_params(data, y,U, C, num_fix, num_group, alpha, mod, num_obs_groups,group_name, name_fix, V=None):

  # initialization of the weights w using k-means
  kmeans_result = KMeans(n_clusters=C, n_init=10)
  kmeans_result.fit(U)
  w = [np.sum(kmeans_result.labels_ == c) / U.shape[0] for c in range(C)]
  params = {"w": w}

  # initialization of mu and sigma
  initial_z = np.zeros((U.shape[0], C), dtype=int)
  mu = kmeans_result.cluster_centers_
  sigma = np.zeros((U.shape[1], U.shape[1], C))
  for c in range(C):
    cluster_indices = np.where(kmeans_result.labels_ == c)
    cluster_data = U.iloc[cluster_indices[0]]
    sigma[:, :, c] = np.cov(cluster_data, rowvar=False)
    initial_z[cluster_indices,c] = 1


  params["mu"] = mu
  params["sigma"] = sigma


  # initialization of lambda
  if V is not None:
    forma = V.shape
    if len(forma) > 1:
      v = forma[1]
    else:
      v = 1

    lambda_list = []

    for c in range(C):
      cluster_indices = np.where(kmeans_result.labels_ == c)
      lambda_c = []
      if v ==1:
        vec = V.iloc[cluster_indices[0]]
        lambda_c.append(np.sum(vec, axis=0) / vec.shape[0])
      else :
        for i in range(v):
          vec = V.iloc[cluster_indices[0], i]
          lambda_c.append(np.sum(vec, axis=0) / vec.shape[0])

      lambda_list.append(lambda_c)

    params["lam"] = lambda_list


  # initialization of parameters of SPGLMM

  fitted_values = []
  fix_param = []
  groups = []
  interc = []
  knots_save = []

  for c in range(C):
    # raggruppare le osservazioni rispetto ai gruppi
    data_c = data[initial_z[:, c] == 1]
    y_c = list(data_c.groupby(group_name)['y'].apply(np.array).values)
    lengths_c = np.array(data_c.groupby(group_name).count().reset_index().iloc[:,1])
    num_group_c = len(data_c.groupby(group_name).count().reset_index()[group_name])
    data_fix = defaultdict(list)
    for i in range(num_fix):
      data_fix[i] = data_c.groupby(group_name)[name_fix[i]].apply(np.array).values.tolist()

    knots, par, W, hess_ran, hess_fix, others = algorithm_alpha(ran_var=False , ran_int=True,
                                                       n_fix=num_fix, sim=False,
                                                       tol=alpha, model=mod,
                                                       fix=data_fix, lengths=lengths_c,
                                                       y=y_c, N=num_group_c, t=None, data=data_c, 
                                                       name_fix=name_fix, name_output=['y'], name_group=group_name)


    fix_param.append(par)
    knots_save.append(knots)

    #creo i gruppi
    group = np.zeros((num_group_c, 1))
    for i in range(num_group_c):
      group[i] = int(np.argmax(W[i, :]))
    groups.append(group)

   #fitted values
    X = pd.DataFrame({'cluster': range(len(knots)), 'knots': knots})
    Y = pd.DataFrame({'Group': data_c.groupby(group_name).count().reset_index()[group_name], 'cluster': group[:,0]})
    Z = Y.merge(X, on=['cluster'])
    data_copy_c = data_c.copy()
    data_copy_c.loc[:, 'Group'] = data_copy_c.loc[:, group_name]
    data_copy_c = data_copy_c.merge(data_copy_c.merge(Z, how='left', on='Group', sort=False))
    interc.append(data_copy_c['knots'])

    if mod == 'B':
      value = data_copy_c['knots'] + np.dot(data_copy_c[name_fix], par)
      fitted_values.append(1 / (1 + np.exp(-value)))
    elif mod == 'P':
      value = data_copy_c['knots'] + np.dot(data_copy_c[name_fix], par)
      fitted_values.append(np.round(np.exp(value)))

  params["rand_inter"] = interc
  params["fix_param"] = fix_param
  params["groups"] = groups
  params["fitted_values"] = fitted_values
  params["z"] = initial_z
  params["knots"] = knots_save

  return params

def E_step(y, U, C, mod, params=None, V=None):

    # Carica i parametri dal dizionario 'params'
    w_old = params["w"]
    mu_old = params["mu"]
    sigma_old = params["sigma"]
    rand_inter_old = params["rand_inter"]
    fix_param_old = params["fix_param"]
    groups_old = params["groups"]
    fitted_values_old = params["fitted_values"]
    z_old = params["z"]
    if V is not None:
      lam_old = params["lam"]

    if V is not None:
      forma = V.shape
      if len(forma) > 1:
         v = forma[1]
      else:
         v = 1
    else:
      v = 0
    multinom_density = np.zeros((len(y), C))

    if V is not None:
        for i in range(C):
          if v == 1:
            multinom_density[z_old[:,i]==1, i] = all_mydmultinom_log(V.iloc[z_old[:,i]==1], lam_old[i], v)
          else:
            multinom_density[z_old[:,i]==1, i] = all_mydmultinom_log(V.iloc[z_old[:,i]==1,:], lam_old[i], v)


    z = np.zeros((U.shape[0], C))

    for i in range(C):
      c_l = np.zeros(len(y))
      if mod == 'B':
        c_l[z_old[:,i]==1] = cross_log(fitted_values_old[i], list(y[z_old[:,i]==1]))
      else :
        c_l[z_old[:,i]==1] = poisson_log_loss(fitted_values_old[i], list(y[z_old[:,i]==1]))
      log_likelihood = (np.log(w_old[i]) + c_l +
                         multivariate_normal.logpdf(U, mean=mu_old[i], cov=sigma_old[:,:,i], allow_singular=True) +
                         multinom_density[:,i])

      z[:, i] = np.exp(log_likelihood)

    for i in range(len(z)):
      divisor =  np.sum(z[i])
      for j in range(len(z[i])):
         z[i][j] = z[i][j] / divisor

    z = np.round(z)
    return z

def M_step(data, y, U, C, z, params, num_fix, num_group, alpha, mod, num_obs_groups, group_name, name_fix, V=None):

  # Carica i parametri dal dizionario 'params'
  w_old = params["w"]
  mu_old = params["mu"]
  sigma_old = params["sigma"]
  rand_inter_old = params["rand_inter"]
  fix_param_old = params["fix_param"]
  groups_old = params["groups"]
  fitted_values_old = params["fitted_values"]

  if V is not None:
    lam_old = params["lam"]

  new_params = {}
  sum_z = np.sum(z, axis=0)

  # update parameter w
  w = sum_z / data.shape[0]
  new_params["w"] = w

  # update parameters related to U
  mu = np.dot(z.T, U)
  for i in range(C):
    mu[i] = mu[i]/sum_z[i]
  sigma = np.zeros((U.shape[1], U.shape[1], C))
  for c in range(C):
    j = U - np.tile(mu[c], (U.shape[0], 1))
    s = mymult3(z[:, c], j, U)
    s = s / sum_z[c]
    sigma[:, :, c] = s
  new_params["mu"] = mu
  new_params["sigma"] = sigma

  # update parameters related to V
  if V is not None:
    forma = V.shape
    if len(forma) > 1:
         v = forma[1]
    else:
         v = 1
    lam = []
    for c in range(C):
      lam_c = []
      if v == 1:
        lam_c.append(mymult4(z[:, c], V) / sum_z[c])
      else:
        for j in range(v):
            lam_c.append(mymult4(z[:, c], V.iloc[:,j]) / sum_z[c])
      lam.append(lam_c)
    new_params["lam"] = lam


  # update parameters related to Y
  fitted_values = []
  fix_param = []
  groups = []
  interc = []
  knots_save = []

  for c in range(C):
    # raggruppare le osservazioni rispetto ai gruppi
    data_c = data[z[:, c] == 1]
    y_c = list(data_c.groupby(group_name)['y'].apply(np.array).values)
    lengths_c = np.array(data_c.groupby(group_name).count().reset_index().iloc[:,1])
    num_group_c = len(data_c.groupby(group_name).count().reset_index()[group_name])
    data_fix = defaultdict(list)
    for i in range(num_fix):
      data_fix[i] = data_c.groupby(group_name)[name_fix[i]].apply(np.array).values.tolist()

    knots, par, W, hess_ran, hess_fix, others = algorithm_alpha(ran_var=False , ran_int=True,
                                                       n_fix=num_fix, sim=False,
                                                       tol=alpha, model=mod,
                                                       fix=data_fix, lengths=lengths_c,
                                                       y=y_c, N=num_group_c, t=None,data=data_c, 
                                                       name_fix=name_fix, name_output=['y'], name_group=group_name)

    fix_param.append(par)
    knots_save.append(knots)

    #creo i gruppi
    group = np.zeros((num_group_c,1))
    for i in range(num_group_c):
      group[i] = int(np.argmax(W[i, :]))
    groups.append(group)

   #fitted values
    X = pd.DataFrame({'cluster': range(len(knots)), 'knots': knots})
    Y = pd.DataFrame({'Group': data_c.groupby(group_name).count().reset_index()[group_name], 'cluster': group[:,0]})
    Z = Y.merge(X, on=['cluster'])
    data_copy_c = data_c.copy()
    data_copy_c.loc[:, 'Group'] = data_copy_c.loc[:, group_name]
    data_copy_c = data_copy_c.merge(data_copy_c.merge(Z, how='left', on='Group', sort=False))
    interc.append(data_copy_c['knots'])

    if mod == 'B':
      value = data_copy_c['knots'] + np.dot(data_copy_c[name_fix], par)
      fitted_values.append(1 / (1 + np.exp(-value)))
    elif mod == 'P':
      value = data_copy_c['knots'] + np.dot(data_copy_c[name_fix], par)
      fitted_values.append(np.round(np.exp(value)))

  new_params["rand_inter"] = interc
  new_params["fix_param"] = fix_param
  new_params["groups"] = groups
  new_params["fitted_values"] = fitted_values
  new_params["z"] = z
  new_params["knots"] = knots_save

  return new_params

def loglikelihood(knots, param_fixed, group, model, n_fix, fix, y, num_group):
        s = []  # s <- rep(0,N)
        # param_fixed = par
        # param_random = knots = c

        for i in range(num_group):

            if group[i]==group[i]:
              a = y[i]

              b = knots[int(group[i])]
              for k in range(n_fix):
                b = b + param_fixed[k] * fix[k][i]

              if model=='B':
                s.append(np.sum(a * b - np.log(1 + np.exp(b.astype(float) ) ) )) # HO CAMBIATO QUI: ho messo .astype(float)
              elif model=='P':
                s.append(np.sum(a * b - np.exp(b.astype(float)) - np.log(np.nan_to_num(factorial(a))) ))

        return np.sum(np.array(s))

def log_like(data, y, U, C, z, params, mod, group_name, name_fix, num_fix, V=None):
  # Carica i parametri dal dizionario 'params'
  w_old = params["w"]
  mu_old = params["mu"]
  sigma_old = params["sigma"]
  rand_inter_old = params["rand_inter"]
  fix_param_old = params["fix_param"]
  groups_old = params["groups"]
  fitted_values_old = params["fitted_values"]
  knots_old = params["knots"]

  if V is not None:
    lam_old = params["lam"]

  # likelihood for V
  multinom_Lik = np.zeros((len(y), C))
  if V is not None:
    forma = V.shape
    if len(forma) > 1:
         v = forma[1]
    else:
         v = 1
    for i in range(C):
         multinom_Lik[z[:,i]==1, i] = all_mydmultinom_log(V[z[:,i]==1], lam_old[i], v)

  # likelihood for U, V, D
  tot = 0
  for c in range(C):
    tot = tot + z[:, c] * (np.log(w_old[c]) +
                          multivariate_normal.logpdf(U, mean=mu_old[c], cov=sigma_old[:, :, c], allow_singular=True) +
                          multinom_Lik[:,c])

  # likelihood for SPGLMM
  lik_SPGLMM = 0
  for c in range(C):
    data_c = data[z[:, c] == 1]
    y_c = list(data_c.groupby(group_name)['y'].apply(np.array).values)
    num_group_c = len(data_c.groupby(group_name).count().reset_index()[group_name])
    data_fix = defaultdict(list)
    for i in range(num_fix):
      data_fix[i] = data_c.groupby(group_name)[name_fix[i]].apply(np.array).values.tolist()

    lik_SPGLMM += loglikelihood(knots_old[c], fix_param_old[c], groups_old[c], mod, num_fix, data_fix, y_c, num_group_c)

  return np.sum(tot) + lik_SPGLMM

def Algo_full(data, y, C, U, num_fix, num_group, alpha, mod, num_obs_groups, group_name, name_fix, max_iter, V=None):
    iter = 0
    params = initialize_params(data, y, U, C, num_fix, num_group, alpha, mod, num_obs_groups, group_name, name_fix, V)
    z = E_step(y, U, C, mod, params, V)
    log_l = [0, log_like(data, y, U, C, z, params, mod, group_name, name_fix, num_fix, V)]
    tol = 1e-3
    loop = False

    while abs(log_l[-1] - log_l[-2]) > tol and iter<max_iter and loop == False:
        params_old = params
        z_old = z

        # E-step
        z = E_step(y, U, C, mod, params, V)

        # M-step
        params = M_step(data, y, U, C, z, params, num_fix, num_group, alpha, mod, num_obs_groups, group_name, name_fix, V)

        # Saving log-likelihood
        new_log_l = log_like(data, y, U, C, z, params, mod, group_name, name_fix, num_fix, V)
        log_l.append(new_log_l)

        iter += 1

        if (iter > 3 and abs(log_l[-1] - log_l[-3]) < tol and abs(log_l[-2] - log_l[-4]) < tol) :
          loop = True
        print("FINE ITERAZIONE", iter)

    if loop == True:
        print("ALGORITMO ENTRATO NEL LOOP")
        if log_l[-1]>log_l[-2] :
          return params, log_l, z, iter
        else :
          return params_old, log_l, z_old, iter

    elif iter>=max_iter:
      print("RAGGIUNTO IL MASSIMO DI ITERAZIONI")
    else :
       print("ALGORITMO ARRIVATO A CONVERGENZA")


    return params, log_l, z, iter



def algo_simulazione_10_GLM(n_obs, n_groups, num_iter):
   accuracy = []
   accuracy_GLM = []
   accuracy_GLMER = []
   auc_model = []
   for j in range(num_iter):
     n_per_group = n_obs/n_groups
     group_ids = np.repeat(np.arange(1, n_groups + 1), n_per_group)
     w = [0.4, 0.3, 0.3]

     # random intercept
     prob = np.repeat(1/n_groups, n_groups)
     interc_1 = np.random.multinomial(n_groups, prob)
     interc_1 = np.minimum(interc_1, 1) # fisso il numero massimo di categorie ottenute (2 + 1)
     interc_2 = np.random.multinomial(n_groups, prob)
     interc_2 = np.minimum(interc_2, 2)
     interc_3 = np.random.multinomial(n_groups, prob)
     interc_3 = np.minimum(interc_3, 2)
     # covariate fisse estratte da una normale
     mean_1 = [-10,-10]
     cov_1 = np.array([[0.7, 0.5], [0.5, 3]])

     mean_2 = [0,0]
     cov_2 = np.array([[2, -1], [-1, 3]])

     mean_3 = [0,-12]
     cov_3 = np.array([[0.5, 0.2], [0.2, 1]])

     data_1 = np.random.multivariate_normal(mean_1, cov_1, int(w[0] * n_obs))
     data_2 = np.random.multivariate_normal(mean_2, cov_2, int(w[1] * n_obs))
     data_3 = np.random.multivariate_normal(mean_3, cov_3, int(w[2] * n_obs))
     data = np.vstack((data_1, data_2, data_3)) # combina i dati

     # Crea un DataFrame
     data = pd.DataFrame(data, columns=['x1', 'x2'])

     # covariata categorica
     categorie = ["A", "B"]
     prob_1 = [0.5,0.5]
     prob_2 = [0.3,0.7]
     prob_3 = [0.6, 0.4]
     cov_cat_1 = np.random.choice(categorie, size=int(w[0] * n_obs), p=prob_1)
     cov_cat_2 = np.random.choice(categorie, size=int(w[1] * n_obs), p=prob_2)
     cov_cat_3 = np.random.choice(categorie, size=int(w[2] * n_obs), p=prob_3)
     cov_cat = np.concatenate((cov_cat_1, cov_cat_2, cov_cat_3)) # combina i dati
     data['cat'] = cov_cat

     # Aggiungi le colonne 'level' e 'latent'
     np.random.shuffle(group_ids)
     data['group'] = group_ids
     data['cluster'] = np.repeat(np.arange(1, len(w) + 1), [int(w[i] * n_obs) for i in range(len(w))])

     # Riordina le colonne
     data = data[['group', 'cluster', 'x1', 'x2','cat']]
     data['cat'] = data['cat'].astype('category')

     data['x1'] = preprocessing.scale(data.x1)
     data['x2'] = preprocessing.scale(data.x2)

     data = pd.get_dummies(data, columns=['cat'], drop_first=True)
     data['cat_B'] = data['cat_B'].astype(int)

     lin_pred = []
     y = []

     for i in range(n_obs):
       if data['cluster'][i] == 1:
         lin_pred.append(interc_1[data['group'][i] - 1] + 2 * data['x1'][i] - 0.4 * data['x2'][i] + 2 * data['cat_B'][i])
       if data['cluster'][i] == 2:
         lin_pred.append(interc_2[data['group'][i] - 1] + 1.8 * data['x1'][i] - 0.6 * data['x2'][i] - 2 * data['cat_B'][i] )
       if data['cluster'][i] == 3:
         lin_pred.append(interc_3[data['group'][i] - 1] - 1.5 * data['x1'][i] - 0.4 * data['x2'][i] - 0.1 * data['cat_B'][i] )
       prob = 1 / (1 + np.exp(-lin_pred[i]))
       y.append(np.random.binomial(1, prob))

     data['lin_pred'] = lin_pred
     data['y'] = y

     params, log_l, z, iter = Algo_full(data = data, y = data['y'], C = 3,
                             U = data[['x1','x2']], V = data['cat_B'], num_fix = 3,
                             num_group = n_groups, alpha = 0.01, mod = 'B',
                             num_obs_groups = np.repeat(n_per_group,n_groups),
                             group_name = 'group', name_fix = ['x1','x2','cat_B'], max_iter=20)
     
     fitted_cluster = np.zeros(n_obs)
     for i in range(n_obs):
            fitted_cluster[i] = np.argmax(params['z'][i]) + 1
     X = pd.DataFrame({'cluster': range(len(params['knots'][0])), 'knots': params['knots'][0]})
     Y = pd.DataFrame({'group': data[fitted_cluster==1].groupby('group').count().reset_index()['group'], 
                       'cluster': params['groups'][0].flatten().astype(int)})
     Z_0 = Y.merge(X, on=['cluster'])
     X = pd.DataFrame({'cluster': range(len(params['knots'][1])), 'knots': params['knots'][1]})
     Y = pd.DataFrame({'group': data[fitted_cluster==2].groupby('group').count().reset_index()['group'], 
                       'cluster': params['groups'][1].flatten().astype(int)})
     Z_1 = Y.merge(X, on=['cluster'])
     X = pd.DataFrame({'cluster': range(len(params['knots'][2])), 'knots': params['knots'][2]})
     Y = pd.DataFrame({'group': data[fitted_cluster==3].groupby('group').count().reset_index()['group'], 
                       'cluster': params['groups'][2].flatten().astype(int)})
     Z_2 = Y.merge(X, on=['cluster'])
    
     data_pred_1 = data.copy()
     data_pred_1 = data_pred_1.merge(Z_0, on=['group'])

     data_pred_2 = data.copy()
     data_pred_2 = data_pred_2.merge(Z_1, on=['group'])

     data_pred_3 = data.copy()
     data_pred_3 = data_pred_3.merge(Z_2, on=['group'])
     
     w = []
     for c in range(3):
          w.append(np.sum(fitted_cluster==(c+1))/len(data['y']))
     weight = []
     weight.append(w[0]*all_mydmultinom(data_pred_1['cat_B'], params['lam'][0],1) * multivariate_normal.pdf(data_pred_1[['x1','x2']], mean=params['mu'][0], cov=params['sigma'][:, :, 0], allow_singular=True))
     weight.append(w[1]*all_mydmultinom(data_pred_2['cat_B'], params['lam'][0],1) * multivariate_normal.pdf(data_pred_2[['x1','x2']], mean=params['mu'][1], cov=params['sigma'][:, :, 1], allow_singular=True))
     weight.append(w[2]*all_mydmultinom(data_pred_3['cat_B'], params['lam'][0],1) * multivariate_normal.pdf(data_pred_3[['x1','x2']], mean=params['mu'][2], cov=params['sigma'][:, :, 2], allow_singular=True))
     
     predicted_probs = []
     for i in range(len(data['y'])):
        sum_weight = weight[0][i] + weight[1][i] + weight[2][i]
        val_1 = np.array( data_pred_1['knots'][i] + params['fix_param'][0][0] * data_pred_1['x1'][i] + params['fix_param'][0][1] * data_pred_1['x2'][i] + params['fix_param'][0][2] * data_pred_1['cat_B'][i])
        pred_1 = weight[0][i]/sum_weight*np.array(expit(val_1))
        val_2 = np.array( data_pred_2['knots'][i] + params['fix_param'][1][0] * data_pred_2['x1'][i] + params['fix_param'][1][1] * data_pred_2['x2'][i] + params['fix_param'][1][2] * data_pred_2['cat_B'][i])
        pred_2 = weight[1][i]/sum_weight*np.array(expit(val_2))
        val_3 = np.array( data_pred_3['knots'][i] + params['fix_param'][2][0] * data_pred_3['x1'][i] + params['fix_param'][2][1] * data_pred_3['x2'][i] + params['fix_param'][2][2] * data_pred_3['cat_B'][i])
        pred_3 = weight[2][i]/sum_weight*np.array(expit(val_3))
        predicted_probs.append(pred_1+pred_2+pred_3)
        
     fpr, tpr, thresholds = roc_curve(data_pred_1['y'], predicted_probs)
     roc_auc = auc(fpr, tpr)
     optimal_threshold_index = np.argmax(tpr - fpr)
     optimal_threshold = thresholds[optimal_threshold_index]
     fitted_values = (np.hstack((predicted_probs)) >= optimal_threshold).astype(int)
     y_per_prev = np.hstack((data_pred_1['y']))
     perc_new = np.count_nonzero(fitted_values == y_per_prev)/len(y_per_prev)
     accuracy.append(perc_new)
     auc_model.append(roc_auc)
     
     data_glm = sm.add_constant(data[['x1','x2','cat_B']])
     glm_model = sm.GLM(data['y'], data_glm, family=sm.families.Binomial())
     glm_result = glm_model.fit()
     fpr, tpr, thresholds_glm = roc_curve(data['y'], glm_result.fittedvalues)
     optimal_threshold_index = np.argmax(tpr - fpr)
     optimal_threshold_glm = thresholds_glm[optimal_threshold_index]
     fitted_glm = (glm_result.fittedvalues >= optimal_threshold_glm).astype(int)
     perc_GLM = np.count_nonzero(fitted_glm == data['y'])/len(data['y'])
     accuracy_GLM.append(perc_GLM)
     
     formula = 'y ~ x1 + x2 + cat_B'
     groups = 'group'
     mixed_model = MixedLM.from_formula(formula, data, groups=groups)
     mixed_result = mixed_model.fit()
     fpr, tpr, thresholds_glmer = roc_curve(data['y'], mixed_result.fittedvalues)
     optimal_threshold_index = np.argmax(tpr - fpr)
     optimal_threshold_glmer = thresholds_glmer[optimal_threshold_index]
     fitted_glmer = (mixed_result.fittedvalues >= optimal_threshold_glmer).astype(int)
     perc_GLMER = np.count_nonzero(fitted_glmer == data['y'])/len(data['y'])
     accuracy_GLMER.append(perc_GLMER)
        
     print("FINE SIMULAZIONE", j)

   return accuracy,accuracy_GLM,accuracy_GLMER,auc_model